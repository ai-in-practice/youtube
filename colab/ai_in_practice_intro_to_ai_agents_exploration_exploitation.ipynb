{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ai-in-practice/youtube/blob/main/colab/ai_in_practice_intro_to_ai_agents_exploration_exploitation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **AI in practice: intro to AI agents - reinforcement learning notebook**\n",
        "\n",
        "Let's get started with reinforcement learning!\n",
        "\n",
        "This notebook contains a simple RL agent that needs to learn how to navigate a frozen lake:\n",
        "\n",
        "<figure>\n",
        "<center>\n",
        "<img src='https://www.gymlibrary.dev/_images/frozen_lake.gif' />\n",
        "<figcaption>Frozen Lake</figcaption></center>\n",
        "</figure>\n",
        "\n",
        "A full description of the `FrozenLake` environment [can be found here](https://www.gymlibrary.dev/environments/toy_text/frozen_lake/).\n",
        "\n",
        "We're using OpenAI's [gym](https://github.com/openai/gym) Python package, which has an extensive collection of environments for reinforcement learning agents.\n",
        "\n",
        "\n",
        "*In order to visualise the agent environments and agent actions in our Google Colab notebooks, we need to install a number of Python and Linux packages. I used a Colab notebook [by Jeff Heaton from Washington University](https://colab.research.google.com/github/jeffheaton/t81_558_deep_learning/blob/master/t81_558_class_12_01_ai_gym.ipynb) to figure out which packages needed to be installed.*"
      ],
      "metadata": {
        "id": "HFvuUXE0ndgk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We first install the necessary Python packages\n"
      ],
      "metadata": {
        "id": "CNeu2FNJnfDL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EyAoOxvvcoPU"
      },
      "outputs": [],
      "source": [
        "!pip install gym pyvirtualdisplay seaborn"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we install the required Linux packages"
      ],
      "metadata": {
        "id": "mWyWydR8oexE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install -y xvfb python-opengl ffmpeg\n",
        "!apt-get update\n",
        "!apt-get install cmake\n"
      ],
      "metadata": {
        "id": "UQBfXPNqnbik"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "With the required Linux packages installed, we need to update the Python runtime environment to be able to run video in the Colab notebook"
      ],
      "metadata": {
        "id": "N_H-BduzonEJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade setuptools\n",
        "!pip install ez_setup\n",
        "!pip install gym[atari]"
      ],
      "metadata": {
        "id": "_Ken05Zbomf4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "With the installation done, we can now import the Python packages into our notebook environment."
      ],
      "metadata": {
        "id": "CwnfYCOko5Au"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import io\n",
        "import gym\n",
        "import glob\n",
        "import base64\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from gym.wrappers.record_video import RecordVideo\n",
        "\n",
        "from IPython.display import HTML\n",
        "from pyvirtualdisplay import Display\n",
        "from IPython import display as ipythondisplay"
      ],
      "metadata": {
        "id": "2aAYDhtQn1ET"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We need some more code to make sure the videos we record are shown in our notebook (adapted from [Jeff Heaton](https://colab.research.google.com/github/jeffheaton/t81_558_deep_learning/blob/master/t81_558_class_12_01_ai_gym.ipynb))"
      ],
      "metadata": {
        "id": "vJ3u0Vehqwfc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def show_video():\n",
        "    mp4list = glob.glob('video/*.mp4')\n",
        "    if len(mp4list) > 0:\n",
        "        mp4 = mp4list[0]\n",
        "        video = io.open(mp4, 'r+b').read()\n",
        "        encoded = base64.b64encode(video)\n",
        "        ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay\n",
        "                loop controls style=\"height: 400px;\">\n",
        "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
        "             </video>'''.format(encoded.decode('ascii'))))\n",
        "    else:\n",
        "        print(\"Could not find video\")"
      ],
      "metadata": {
        "id": "htz-y0rgpV7Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def wrap_env(env):\n",
        "    env = RecordVideo(env, './video')\n",
        "    return env"
      ],
      "metadata": {
        "id": "FrfTOYgkp_f4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "5wjabPgTqnEo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We need to add some code to evaluate how our agent is doing in reaching the treasure on the other side of the frozen lake."
      ],
      "metadata": {
        "id": "ixwRiEchrJ6V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_agent(env, q_table, num_episodes=100):\n",
        "    total_rewards = 0\n",
        "    for _ in range(num_episodes):\n",
        "        state = env.reset()\n",
        "        done = False\n",
        "        while not done:\n",
        "            action = np.argmax(q_table[state, :])\n",
        "            state, reward, done, _ = env.step(action)\n",
        "            total_rewards += reward\n",
        "    return total_rewards / num_episodes"
      ],
      "metadata": {
        "id": "gFXEAKW2tNOH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def visualize_policy(q_table):\n",
        "    policy = np.argmax(q_table, axis=1)\n",
        "    actions = [\"←\", \"↓\", \"→\", \"↑\"]\n",
        "    policy_chars = [actions[a] for a in policy]\n",
        "    policy_chars = np.array(policy_chars).reshape(4, 4)\n",
        "    print(policy_chars)"
      ],
      "metadata": {
        "id": "hCgHf0JxzgJi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Now we can finally run our environment! Let's see what we have going on:**"
      ],
      "metadata": {
        "id": "nqjkfz4LrCAC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "env = gym.make('FrozenLake-v1', is_slippery=False)"
      ],
      "metadata": {
        "id": "VWIG2u9zuRt5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is an environment in which the agent can take four different actions:\n",
        "\n",
        "* 0: LEFT\n",
        "* 1: DOWN\n",
        "* 2: RIGHT\n",
        "* 3: UP\n",
        "\n",
        "For a full description, see the [environment documentation](https://www.gymlibrary.dev/environments/toy_text/frozen_lake/)."
      ],
      "metadata": {
        "id": "KZUy2WeOuiOS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "q_table_before_training = np.zeros([env.observation_space.n, env.action_space.n])"
      ],
      "metadata": {
        "id": "5tZyRyiPtmFr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(env.observation_space.n)"
      ],
      "metadata": {
        "id": "5azCyTk1qK0h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(q_table_before_training)"
      ],
      "metadata": {
        "id": "d8qoT4LOzj6g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We initialise the Q-table with a probability of zero for each of the four actions the can take (left, down, right, up).\n",
        "\n",
        "Rather than taking random actions, for now we'll initialise with a simple rule: 'always go left'."
      ],
      "metadata": {
        "id": "OqYTCDy2tr8H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "visualize_policy(q_table_before_training)"
      ],
      "metadata": {
        "id": "4w2YzL8_1Y1_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's see what this looks like on video"
      ],
      "metadata": {
        "id": "_38qz7FQ0uA8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "video_env = wrap_env(env)"
      ],
      "metadata": {
        "id": "gTuKS_1m1aVE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "observation = video_env.reset()\n",
        "\n",
        "while True:\n",
        "    video_env.render()\n",
        "    action = 0\n",
        "    observation, reward, done, info = video_env.step(action)\n",
        "\n",
        "    if done:\n",
        "        break\n",
        "\n",
        "video_env.close()\n",
        "show_video()"
      ],
      "metadata": {
        "id": "p6JJCMbFzxmr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Not great! :) The agent is just trying to go to the left each step.."
      ],
      "metadata": {
        "id": "rzGjMdbH0xbD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pre_training_score = evaluate_agent(env, q_table_before_training)\n",
        "print(f\"Average reward before training: {pre_training_score}\")"
      ],
      "metadata": {
        "id": "jccSp-Httdm3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As expected, the agent never reaches its goal - the treasure! - and reward is always 0."
      ],
      "metadata": {
        "id": "QUm_JE5A1N0h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Okay, so let's see if we can improve on this by training the agent using Q-learning.\n",
        "\n",
        "### **Q-learning algorithm**\n",
        "\n",
        "The Q-learning algorithm works by updating its value policy (Q) in the following way:\n",
        "\n",
        "$$Q(s, a) \\leftarrow Q(s, a) + \\alpha \\left( r + \\gamma \\max_{a'} Q(s', a') - Q(s, a) \\right)$$\n",
        "\n",
        "where $Q(s,a)$ is the Q-value for state 𝑠 and action 𝑎, α is the learning rate, 𝑟 is the reward received after taking action 𝑎 from state 𝑠, 𝛾 is the discount factor, and $\\max_{𝑎′}𝑄(𝑠′,𝑎′)$ is the maximum Q-value for the next state 𝑠′ over all possible actions 𝑎′.\n",
        "\n",
        "We're setting the hyperparameters based on values that have been shown to work well for Q-learning:\n",
        "\n",
        "* Learning rate (α / alpha): Determines how much new information overrides the old information.\n",
        "* Discount factor (γ / gamma): Determines the importance of future rewards.\n",
        "* Exploration rate (ε / epsilon): Determines the probability of choosing a random action over the best-known action."
      ],
      "metadata": {
        "id": "cCLx6UTRtOHx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the environment\n",
        "env = gym.make('FrozenLake-v1', is_slippery=False)\n",
        "\n",
        "# Initialize Q-table\n",
        "q_table = np.zeros([env.observation_space.n, env.action_space.n])\n",
        "\n",
        "# Hyperparameters\n",
        "alpha = 0.8\n",
        "gamma = 0.95\n",
        "epsilon = 0.2  # Increased epsilon for more exploration\n",
        "num_episodes = 1000\n",
        "max_steps = 100  # Maximum steps per episode\n",
        "\n",
        "# Keep track of the agent task completion rates\n",
        "rewards = []\n",
        "success_rate = []\n",
        "step_count = []\n",
        "\n",
        "for episode in range(num_episodes):\n",
        "    state = env.reset()\n",
        "    if isinstance(state, tuple):\n",
        "        state = state[0]\n",
        "\n",
        "    done = False\n",
        "    total_reward = 0 # A reward is '1' if the agent reaches the treasure, 0 otherwise\n",
        "    steps = 0 # The number of steps the agent needs to reach the treasure\n",
        "\n",
        "    while not done:\n",
        "        # Choose action\n",
        "        if np.random.uniform(0, 1) < epsilon:\n",
        "            # Exploration: choose a random action from 0-3 (up, left, right, down)\n",
        "            action = env.action_space.sample()\n",
        "        else:\n",
        "            # Exploitation: use the learnt policy to select the next action\n",
        "            action = np.argmax(q_table[state, :])\n",
        "\n",
        "        # Take action\n",
        "        step_return = env.step(action)\n",
        "        if len(step_return) == 4:\n",
        "            next_state, reward, done, _ = step_return\n",
        "        else:\n",
        "            next_state, reward, done, _, _ = step_return\n",
        "\n",
        "        # Set up the reward structure\n",
        "        if done and reward == 1:  # Reached the goal - found the treasure!\n",
        "            adjusted_reward = 1\n",
        "            step_count.append(steps + 1)\n",
        "        elif done and reward == 0:  # Fell into a hole in the ice - game over!\n",
        "            adjusted_reward = -1\n",
        "        else:  # Small penalty for each step, continue playing\n",
        "            adjusted_reward = -0.01\n",
        "\n",
        "        total_reward += adjusted_reward\n",
        "\n",
        "        old_q = q_table[state, action]\n",
        "        next_max = np.max(q_table[next_state])\n",
        "        new_q = (1 - alpha) * old_q + alpha * (adjusted_reward + gamma * next_max)\n",
        "        q_table[state, action] = new_q\n",
        "\n",
        "        state = next_state\n",
        "        steps += 1\n",
        "\n",
        "    rewards.append(total_reward)\n",
        "    success_rate.append(1 if total_reward > 0 else 0)\n",
        "\n",
        "\n",
        "# Set up the plotting style\n",
        "plt.style.use('seaborn')\n",
        "sns.set_palette(\"deep\")\n",
        "\n",
        "# Create a figure with two subplots\n",
        "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 10))\n",
        "fig.suptitle('Q-Learning Performance on FrozenLake', fontsize=16)\n",
        "\n",
        "# Plot rewards as a bar chart\n",
        "successful_mask = np.array(rewards) > 0\n",
        "ax1.bar(range(num_episodes), rewards, color=['#2ecc71' if s else '#e74c3c' for s in successful_mask], alpha=0.7)\n",
        "ax1.set_title('Rewards over Episodes', fontsize=14)\n",
        "ax1.set_xlabel('Episode', fontsize=12)\n",
        "ax1.set_ylabel('Total Reward', fontsize=12)\n",
        "ax1.grid(True, linestyle='--', alpha=0.7)\n",
        "\n",
        "# Add a legend for the bar chart\n",
        "from matplotlib.patches import Patch\n",
        "legend_elements = [Patch(facecolor='#2ecc71', edgecolor='#2ecc71', label='Successful'),\n",
        "                   Patch(facecolor='#e74c3c', edgecolor='#e74c3c', label='Unsuccessful')]\n",
        "ax1.legend(handles=legend_elements, loc='upper right')\n",
        "\n",
        "# Plot step count for successful runs\n",
        "ax2.plot(step_count, alpha=0.7)\n",
        "ax2.set_title('Steps for Successful Runs', fontsize=14)\n",
        "ax2.set_xlabel('Successful Run', fontsize=12)\n",
        "ax2.set_ylabel('Number of Steps', fontsize=12)\n",
        "ax2.grid(True, linestyle='--', alpha=0.7)\n",
        "\n",
        "# Add a trend line to the step count plot\n",
        "z = np.polyfit(range(len(step_count)), step_count, 1)\n",
        "p = np.poly1d(z)\n",
        "ax2.plot(range(len(step_count)), p(range(len(step_count))), \"r--\", alpha=0.8)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Print final Q-table\n",
        "print(\"\\nFinal Q-table:\")\n",
        "print(q_table)\n",
        "\n",
        "env.close()"
      ],
      "metadata": {
        "id": "vajuV2V8xe8d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's see how the agent is performing after training"
      ],
      "metadata": {
        "id": "fog-p8T2weUM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "post_training_score = evaluate_agent(env, q_table)\n",
        "print(f\"Average reward after training: {post_training_score}\")\n"
      ],
      "metadata": {
        "id": "HdOAfJ4kc9LI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "That's a lot better! The agent now almost always reaches the treasure 🧳!"
      ],
      "metadata": {
        "id": "kXf5gLKPUz8N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def visualize_policy(q_table):\n",
        "    policy = np.argmax(q_table, axis=1)\n",
        "    actions = [\"←\", \"↓\", \"→\", \"↑\"]\n",
        "    policy_chars = [actions[a] for a in policy]\n",
        "    policy_chars = np.array(policy_chars).reshape(4, 4)\n",
        "    print(policy_chars)\n",
        "\n",
        "visualize_policy(q_table)"
      ],
      "metadata": {
        "id": "ultIdTW1dAYw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "demo_env = wrap_env(env)"
      ],
      "metadata": {
        "id": "ANtMtke2rS9L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "state = demo_env.reset()\n",
        "\n",
        "if isinstance(state, tuple):\n",
        "    state = state[0]\n",
        "\n",
        "done = False\n",
        "num_steps = 0\n",
        "\n",
        "while not done:\n",
        "    action = np.argmax(q_table[state, :])\n",
        "    step_return = demo_env.step(action)\n",
        "    if len(step_return) == 4:\n",
        "        state, reward, done, _ = step_return\n",
        "    else:\n",
        "        state, reward, done, _, _ = step_return\n",
        "    num_steps += 1\n",
        "\n",
        "demo_env.close()\n",
        "\n",
        "if reward == 1:\n",
        "    print(f'The agent reached the treasure in {num_steps} steps!')\n",
        "else:\n",
        "    print(f'Game over! The agent fell into a hole after {num_steps} steps!')\n",
        "show_video()"
      ],
      "metadata": {
        "id": "RaMDJQLZqNuX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "All right! Looks like we've just trained our first AI agent, congratulations!"
      ],
      "metadata": {
        "id": "MEMQ2MUXQHTT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Feel free to play around with the environments, and look at some of the example agents implemented in the [pratical RL course](https://github.com/VinF/practical_sessions_RL/tree/main) for inspiration!"
      ],
      "metadata": {
        "id": "PfAN4OiLR2pG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Happy hacking!"
      ],
      "metadata": {
        "id": "gqYCJmu1RSC4"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pNROmhP4Rzjd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}